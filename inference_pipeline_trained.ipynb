{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "47adb76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from typing import List, Optional\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "from PIL import Image\n",
    "\n",
    "from sahi.models.base import DetectionModel\n",
    "from sahi.prediction import ObjectPrediction\n",
    "from sahi.utils.cv import get_bbox_from_bool_mask, get_coco_segmentation_from_bool_mask\n",
    "from sahi.utils.import_utils import check_requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b128db7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8c0fd94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6007458c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectron2.data import MetadataCatalog\n",
    "from detectron2.engine.defaults import DefaultPredictor\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.projects.deeplab import add_deeplab_config\n",
    "\n",
    "from detectron2.config import LazyConfig, instantiate\n",
    "from detectron2.checkpoint import DetectionCheckpointer\n",
    "import detectron2.data.transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d46ec81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f8a39c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = get_cfg()\n",
    "cfg.set_new_allowed(True) \n",
    "cfg = LazyConfig.load(\"/home/mrajaraman/master-thesis-dragonfly/external/mask-rcnn-dragonfly/configs/new_baselines/mask_rcnn_R_50_FPN_100ep_LSJ.py\") \n",
    "# cfg.train.init_checkpoint = \"/h/jquinto/Mask-RCNN/model_final_14d201.pkl\"\n",
    "\n",
    "# set model device\n",
    "# cfg.MODEL.DEVICE = \"self.device.type\"\n",
    "cfg.train.device = \"cuda\"\n",
    "\n",
    "# # set input image size\n",
    "# # NEW TRAINING PIPELINE\n",
    "# cfg.INPUT.MIN_SIZE_TEST = 1024\n",
    "# cfg.INPUT.MAX_SIZE_TEST = 1024\n",
    "# cfg.freeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "56e98be4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09/19/2025 20:51:10 - INFO - detectron2.checkpoint.detection_checkpoint -   [DetectionCheckpointer] Loading from /home/mrajaraman/master-thesis-dragonfly/external/mask2former-dragonfly/output_lifeplan_b_512_sahi_tiled_v9_R50_1024_one_cycle_lr_5e-5_colour_augs_15k_iters/model_final.pth ...\n",
      "09/19/2025 20:51:10 - INFO - fvcore.common.checkpoint -   [Checkpointer] Loading from /home/mrajaraman/master-thesis-dragonfly/external/mask2former-dragonfly/output_lifeplan_b_512_sahi_tiled_v9_R50_1024_one_cycle_lr_5e-5_colour_augs_15k_iters/model_final.pth ...\n",
      "09/19/2025 20:51:10 - WARNING - fvcore.common.checkpoint -   Some model parameters or buffers are not found in the checkpoint:\n",
      "\u001b[34mbackbone.bottom_up.res2.0.conv1.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res2.0.conv1.weight\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res2.0.conv2.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res2.0.conv2.weight\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res2.0.conv3.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res2.0.conv3.weight\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res2.0.shortcut.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res2.0.shortcut.weight\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res2.1.conv1.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res2.1.conv1.weight\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res2.1.conv2.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res2.1.conv2.weight\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res2.1.conv3.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res2.1.conv3.weight\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res2.2.conv1.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res2.2.conv1.weight\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res2.2.conv2.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res2.2.conv2.weight\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res2.2.conv3.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res2.2.conv3.weight\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res3.0.conv1.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res3.0.conv1.weight\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res3.0.conv2.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res3.0.conv2.weight\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res3.0.conv3.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res3.0.conv3.weight\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res3.0.shortcut.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res3.0.shortcut.weight\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res3.1.conv1.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res3.1.conv1.weight\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res3.1.conv2.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res3.1.conv2.weight\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res3.1.conv3.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res3.1.conv3.weight\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res3.2.conv1.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res3.2.conv1.weight\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res3.2.conv2.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res3.2.conv2.weight\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res3.2.conv3.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res3.2.conv3.weight\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res3.3.conv1.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res3.3.conv1.weight\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res3.3.conv2.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res3.3.conv2.weight\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res3.3.conv3.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res3.3.conv3.weight\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res4.0.conv1.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res4.0.conv1.weight\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res4.0.conv2.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res4.0.conv2.weight\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res4.0.conv3.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res4.0.conv3.weight\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res4.0.shortcut.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res4.0.shortcut.weight\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res4.1.conv1.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res4.1.conv1.weight\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res4.1.conv2.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res4.1.conv2.weight\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res4.1.conv3.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res4.1.conv3.weight\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res4.2.conv1.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res4.2.conv1.weight\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res4.2.conv2.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res4.2.conv2.weight\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res4.2.conv3.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res4.2.conv3.weight\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res4.3.conv1.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res4.3.conv1.weight\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res4.3.conv2.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res4.3.conv2.weight\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res4.3.conv3.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res4.3.conv3.weight\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res4.4.conv1.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res4.4.conv1.weight\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res4.4.conv2.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res4.4.conv2.weight\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res4.4.conv3.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res4.4.conv3.weight\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res4.5.conv1.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res4.5.conv1.weight\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res4.5.conv2.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res4.5.conv2.weight\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res4.5.conv3.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res4.5.conv3.weight\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res5.0.conv1.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res5.0.conv1.weight\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res5.0.conv2.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res5.0.conv2.weight\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res5.0.conv3.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res5.0.conv3.weight\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res5.0.shortcut.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res5.0.shortcut.weight\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res5.1.conv1.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res5.1.conv1.weight\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res5.1.conv2.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res5.1.conv2.weight\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res5.1.conv3.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res5.1.conv3.weight\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res5.2.conv1.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res5.2.conv1.weight\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res5.2.conv2.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res5.2.conv2.weight\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res5.2.conv3.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res5.2.conv3.weight\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.stem.conv1.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.stem.conv1.weight\u001b[0m\n",
      "\u001b[34mbackbone.fpn_lateral2.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "\u001b[34mbackbone.fpn_lateral2.weight\u001b[0m\n",
      "\u001b[34mbackbone.fpn_lateral3.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "\u001b[34mbackbone.fpn_lateral3.weight\u001b[0m\n",
      "\u001b[34mbackbone.fpn_lateral4.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "\u001b[34mbackbone.fpn_lateral4.weight\u001b[0m\n",
      "\u001b[34mbackbone.fpn_lateral5.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "\u001b[34mbackbone.fpn_lateral5.weight\u001b[0m\n",
      "\u001b[34mbackbone.fpn_output2.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "\u001b[34mbackbone.fpn_output2.weight\u001b[0m\n",
      "\u001b[34mbackbone.fpn_output3.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "\u001b[34mbackbone.fpn_output3.weight\u001b[0m\n",
      "\u001b[34mbackbone.fpn_output4.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "\u001b[34mbackbone.fpn_output4.weight\u001b[0m\n",
      "\u001b[34mbackbone.fpn_output5.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "\u001b[34mbackbone.fpn_output5.weight\u001b[0m\n",
      "\u001b[34mproposal_generator.rpn_head.anchor_deltas.{bias, weight}\u001b[0m\n",
      "\u001b[34mproposal_generator.rpn_head.conv.conv0.{bias, weight}\u001b[0m\n",
      "\u001b[34mproposal_generator.rpn_head.conv.conv1.{bias, weight}\u001b[0m\n",
      "\u001b[34mproposal_generator.rpn_head.objectness_logits.{bias, weight}\u001b[0m\n",
      "\u001b[34mroi_heads.box_head.conv1.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "\u001b[34mroi_heads.box_head.conv1.weight\u001b[0m\n",
      "\u001b[34mroi_heads.box_head.conv2.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "\u001b[34mroi_heads.box_head.conv2.weight\u001b[0m\n",
      "\u001b[34mroi_heads.box_head.conv3.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "\u001b[34mroi_heads.box_head.conv3.weight\u001b[0m\n",
      "\u001b[34mroi_heads.box_head.conv4.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "\u001b[34mroi_heads.box_head.conv4.weight\u001b[0m\n",
      "\u001b[34mroi_heads.box_head.fc1.{bias, weight}\u001b[0m\n",
      "\u001b[34mroi_heads.box_predictor.bbox_pred.{bias, weight}\u001b[0m\n",
      "\u001b[34mroi_heads.box_predictor.cls_score.{bias, weight}\u001b[0m\n",
      "\u001b[34mroi_heads.mask_head.deconv.{bias, weight}\u001b[0m\n",
      "\u001b[34mroi_heads.mask_head.mask_fcn1.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "\u001b[34mroi_heads.mask_head.mask_fcn1.weight\u001b[0m\n",
      "\u001b[34mroi_heads.mask_head.mask_fcn2.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "\u001b[34mroi_heads.mask_head.mask_fcn2.weight\u001b[0m\n",
      "\u001b[34mroi_heads.mask_head.mask_fcn3.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "\u001b[34mroi_heads.mask_head.mask_fcn3.weight\u001b[0m\n",
      "\u001b[34mroi_heads.mask_head.mask_fcn4.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "\u001b[34mroi_heads.mask_head.mask_fcn4.weight\u001b[0m\n",
      "\u001b[34mroi_heads.mask_head.predictor.{bias, weight}\u001b[0m\n",
      "09/19/2025 20:51:10 - WARNING - fvcore.common.checkpoint -   The checkpoint state_dict contains keys that are not used by the model:\n",
      "  \u001b[35msem_seg_head.pixel_decoder.input_proj.0.0.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.pixel_decoder.input_proj.0.1.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.pixel_decoder.input_proj.1.0.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.pixel_decoder.input_proj.1.1.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.pixel_decoder.input_proj.2.0.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.pixel_decoder.input_proj.2.1.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.pixel_decoder.transformer.level_embed\u001b[0m\n",
      "  \u001b[35msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.sampling_offsets.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.attention_weights.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.value_proj.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.output_proj.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.pixel_decoder.transformer.encoder.layers.0.norm1.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.pixel_decoder.transformer.encoder.layers.0.linear1.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.pixel_decoder.transformer.encoder.layers.0.linear2.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.pixel_decoder.transformer.encoder.layers.0.norm2.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.sampling_offsets.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.attention_weights.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.value_proj.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.output_proj.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.pixel_decoder.transformer.encoder.layers.1.norm1.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.pixel_decoder.transformer.encoder.layers.1.linear1.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.pixel_decoder.transformer.encoder.layers.1.linear2.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.pixel_decoder.transformer.encoder.layers.1.norm2.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.sampling_offsets.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.attention_weights.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.value_proj.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.output_proj.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.pixel_decoder.transformer.encoder.layers.2.norm1.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.pixel_decoder.transformer.encoder.layers.2.linear1.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.pixel_decoder.transformer.encoder.layers.2.linear2.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.pixel_decoder.transformer.encoder.layers.2.norm2.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.sampling_offsets.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.attention_weights.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.value_proj.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.output_proj.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.pixel_decoder.transformer.encoder.layers.3.norm1.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.pixel_decoder.transformer.encoder.layers.3.linear1.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.pixel_decoder.transformer.encoder.layers.3.linear2.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.pixel_decoder.transformer.encoder.layers.3.norm2.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.sampling_offsets.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.attention_weights.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.value_proj.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.output_proj.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.pixel_decoder.transformer.encoder.layers.4.norm1.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.pixel_decoder.transformer.encoder.layers.4.linear1.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.pixel_decoder.transformer.encoder.layers.4.linear2.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.pixel_decoder.transformer.encoder.layers.4.norm2.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.sampling_offsets.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.attention_weights.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.value_proj.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.output_proj.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.pixel_decoder.transformer.encoder.layers.5.norm1.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.pixel_decoder.transformer.encoder.layers.5.linear1.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.pixel_decoder.transformer.encoder.layers.5.linear2.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.pixel_decoder.transformer.encoder.layers.5.norm2.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.pixel_decoder.mask_features.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.pixel_decoder.adapter_1.weight\u001b[0m\n",
      "  \u001b[35msem_seg_head.pixel_decoder.adapter_1.norm.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.pixel_decoder.layer_1.weight\u001b[0m\n",
      "  \u001b[35msem_seg_head.pixel_decoder.layer_1.norm.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.predictor.transformer_self_attention_layers.0.self_attn.{in_proj_bias, in_proj_weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.predictor.transformer_self_attention_layers.0.self_attn.out_proj.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.predictor.transformer_self_attention_layers.0.norm.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.predictor.transformer_self_attention_layers.1.self_attn.{in_proj_bias, in_proj_weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.predictor.transformer_self_attention_layers.1.self_attn.out_proj.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.predictor.transformer_self_attention_layers.1.norm.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.predictor.transformer_self_attention_layers.2.self_attn.{in_proj_bias, in_proj_weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.predictor.transformer_self_attention_layers.2.self_attn.out_proj.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.predictor.transformer_self_attention_layers.2.norm.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.predictor.transformer_self_attention_layers.3.self_attn.{in_proj_bias, in_proj_weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.predictor.transformer_self_attention_layers.3.self_attn.out_proj.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.predictor.transformer_self_attention_layers.3.norm.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.predictor.transformer_self_attention_layers.4.self_attn.{in_proj_bias, in_proj_weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.predictor.transformer_self_attention_layers.4.self_attn.out_proj.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.predictor.transformer_self_attention_layers.4.norm.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.predictor.transformer_self_attention_layers.5.self_attn.{in_proj_bias, in_proj_weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.predictor.transformer_self_attention_layers.5.self_attn.out_proj.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.predictor.transformer_self_attention_layers.5.norm.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.predictor.transformer_self_attention_layers.6.self_attn.{in_proj_bias, in_proj_weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.predictor.transformer_self_attention_layers.6.self_attn.out_proj.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.predictor.transformer_self_attention_layers.6.norm.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.predictor.transformer_self_attention_layers.7.self_attn.{in_proj_bias, in_proj_weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.predictor.transformer_self_attention_layers.7.self_attn.out_proj.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.predictor.transformer_self_attention_layers.7.norm.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.predictor.transformer_self_attention_layers.8.self_attn.{in_proj_bias, in_proj_weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.predictor.transformer_self_attention_layers.8.self_attn.out_proj.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.predictor.transformer_self_attention_layers.8.norm.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.predictor.transformer_cross_attention_layers.0.multihead_attn.{in_proj_bias, in_proj_weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.predictor.transformer_cross_attention_layers.0.multihead_attn.out_proj.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.predictor.transformer_cross_attention_layers.0.norm.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.predictor.transformer_cross_attention_layers.1.multihead_attn.{in_proj_bias, in_proj_weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.predictor.transformer_cross_attention_layers.1.multihead_attn.out_proj.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.predictor.transformer_cross_attention_layers.1.norm.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.predictor.transformer_cross_attention_layers.2.multihead_attn.{in_proj_bias, in_proj_weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.predictor.transformer_cross_attention_layers.2.multihead_attn.out_proj.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.predictor.transformer_cross_attention_layers.2.norm.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.predictor.transformer_cross_attention_layers.3.multihead_attn.{in_proj_bias, in_proj_weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.predictor.transformer_cross_attention_layers.3.multihead_attn.out_proj.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.predictor.transformer_cross_attention_layers.3.norm.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.predictor.transformer_cross_attention_layers.4.multihead_attn.{in_proj_bias, in_proj_weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.predictor.transformer_cross_attention_layers.4.multihead_attn.out_proj.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.predictor.transformer_cross_attention_layers.4.norm.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.predictor.transformer_cross_attention_layers.5.multihead_attn.{in_proj_bias, in_proj_weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.predictor.transformer_cross_attention_layers.5.multihead_attn.out_proj.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.predictor.transformer_cross_attention_layers.5.norm.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.predictor.transformer_cross_attention_layers.6.multihead_attn.{in_proj_bias, in_proj_weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.predictor.transformer_cross_attention_layers.6.multihead_attn.out_proj.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.predictor.transformer_cross_attention_layers.6.norm.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.predictor.transformer_cross_attention_layers.7.multihead_attn.{in_proj_bias, in_proj_weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.predictor.transformer_cross_attention_layers.7.multihead_attn.out_proj.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.predictor.transformer_cross_attention_layers.7.norm.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.predictor.transformer_cross_attention_layers.8.multihead_attn.{in_proj_bias, in_proj_weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.predictor.transformer_cross_attention_layers.8.multihead_attn.out_proj.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.predictor.transformer_cross_attention_layers.8.norm.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.predictor.transformer_ffn_layers.0.linear1.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.predictor.transformer_ffn_layers.0.linear2.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.predictor.transformer_ffn_layers.0.norm.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.predictor.transformer_ffn_layers.1.linear1.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.predictor.transformer_ffn_layers.1.linear2.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.predictor.transformer_ffn_layers.1.norm.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.predictor.transformer_ffn_layers.2.linear1.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.predictor.transformer_ffn_layers.2.linear2.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.predictor.transformer_ffn_layers.2.norm.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.predictor.transformer_ffn_layers.3.linear1.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.predictor.transformer_ffn_layers.3.linear2.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.predictor.transformer_ffn_layers.3.norm.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.predictor.transformer_ffn_layers.4.linear1.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.predictor.transformer_ffn_layers.4.linear2.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.predictor.transformer_ffn_layers.4.norm.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.predictor.transformer_ffn_layers.5.linear1.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.predictor.transformer_ffn_layers.5.linear2.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.predictor.transformer_ffn_layers.5.norm.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.predictor.transformer_ffn_layers.6.linear1.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.predictor.transformer_ffn_layers.6.linear2.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.predictor.transformer_ffn_layers.6.norm.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.predictor.transformer_ffn_layers.7.linear1.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.predictor.transformer_ffn_layers.7.linear2.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.predictor.transformer_ffn_layers.7.norm.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.predictor.transformer_ffn_layers.8.linear1.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.predictor.transformer_ffn_layers.8.linear2.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.predictor.transformer_ffn_layers.8.norm.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.predictor.decoder_norm.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.predictor.query_feat.weight\u001b[0m\n",
      "  \u001b[35msem_seg_head.predictor.query_embed.weight\u001b[0m\n",
      "  \u001b[35msem_seg_head.predictor.level_embed.weight\u001b[0m\n",
      "  \u001b[35msem_seg_head.predictor.class_embed.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.predictor.mask_embed.layers.0.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.predictor.mask_embed.layers.1.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.predictor.mask_embed.layers.2.{bias, weight}\u001b[0m\n",
      "  \u001b[35mcriterion.empty_weight\u001b[0m\n",
      "  \u001b[35mbackbone.stem.conv1.weight\u001b[0m\n",
      "  \u001b[35mbackbone.stem.conv1.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.res2.0.shortcut.weight\u001b[0m\n",
      "  \u001b[35mbackbone.res2.0.shortcut.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.res2.0.conv1.weight\u001b[0m\n",
      "  \u001b[35mbackbone.res2.0.conv1.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.res2.0.conv2.weight\u001b[0m\n",
      "  \u001b[35mbackbone.res2.0.conv2.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.res2.0.conv3.weight\u001b[0m\n",
      "  \u001b[35mbackbone.res2.0.conv3.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.res2.1.conv1.weight\u001b[0m\n",
      "  \u001b[35mbackbone.res2.1.conv1.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.res2.1.conv2.weight\u001b[0m\n",
      "  \u001b[35mbackbone.res2.1.conv2.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.res2.1.conv3.weight\u001b[0m\n",
      "  \u001b[35mbackbone.res2.1.conv3.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.res2.2.conv1.weight\u001b[0m\n",
      "  \u001b[35mbackbone.res2.2.conv1.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.res2.2.conv2.weight\u001b[0m\n",
      "  \u001b[35mbackbone.res2.2.conv2.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.res2.2.conv3.weight\u001b[0m\n",
      "  \u001b[35mbackbone.res2.2.conv3.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.res3.0.shortcut.weight\u001b[0m\n",
      "  \u001b[35mbackbone.res3.0.shortcut.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.res3.0.conv1.weight\u001b[0m\n",
      "  \u001b[35mbackbone.res3.0.conv1.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.res3.0.conv2.weight\u001b[0m\n",
      "  \u001b[35mbackbone.res3.0.conv2.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.res3.0.conv3.weight\u001b[0m\n",
      "  \u001b[35mbackbone.res3.0.conv3.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.res3.1.conv1.weight\u001b[0m\n",
      "  \u001b[35mbackbone.res3.1.conv1.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.res3.1.conv2.weight\u001b[0m\n",
      "  \u001b[35mbackbone.res3.1.conv2.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.res3.1.conv3.weight\u001b[0m\n",
      "  \u001b[35mbackbone.res3.1.conv3.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.res3.2.conv1.weight\u001b[0m\n",
      "  \u001b[35mbackbone.res3.2.conv1.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.res3.2.conv2.weight\u001b[0m\n",
      "  \u001b[35mbackbone.res3.2.conv2.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.res3.2.conv3.weight\u001b[0m\n",
      "  \u001b[35mbackbone.res3.2.conv3.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.res3.3.conv1.weight\u001b[0m\n",
      "  \u001b[35mbackbone.res3.3.conv1.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.res3.3.conv2.weight\u001b[0m\n",
      "  \u001b[35mbackbone.res3.3.conv2.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.res3.3.conv3.weight\u001b[0m\n",
      "  \u001b[35mbackbone.res3.3.conv3.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.res4.0.shortcut.weight\u001b[0m\n",
      "  \u001b[35mbackbone.res4.0.shortcut.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.res4.0.conv1.weight\u001b[0m\n",
      "  \u001b[35mbackbone.res4.0.conv1.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.res4.0.conv2.weight\u001b[0m\n",
      "  \u001b[35mbackbone.res4.0.conv2.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.res4.0.conv3.weight\u001b[0m\n",
      "  \u001b[35mbackbone.res4.0.conv3.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.res4.1.conv1.weight\u001b[0m\n",
      "  \u001b[35mbackbone.res4.1.conv1.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.res4.1.conv2.weight\u001b[0m\n",
      "  \u001b[35mbackbone.res4.1.conv2.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.res4.1.conv3.weight\u001b[0m\n",
      "  \u001b[35mbackbone.res4.1.conv3.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.res4.2.conv1.weight\u001b[0m\n",
      "  \u001b[35mbackbone.res4.2.conv1.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.res4.2.conv2.weight\u001b[0m\n",
      "  \u001b[35mbackbone.res4.2.conv2.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.res4.2.conv3.weight\u001b[0m\n",
      "  \u001b[35mbackbone.res4.2.conv3.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.res4.3.conv1.weight\u001b[0m\n",
      "  \u001b[35mbackbone.res4.3.conv1.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.res4.3.conv2.weight\u001b[0m\n",
      "  \u001b[35mbackbone.res4.3.conv2.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.res4.3.conv3.weight\u001b[0m\n",
      "  \u001b[35mbackbone.res4.3.conv3.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.res4.4.conv1.weight\u001b[0m\n",
      "  \u001b[35mbackbone.res4.4.conv1.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.res4.4.conv2.weight\u001b[0m\n",
      "  \u001b[35mbackbone.res4.4.conv2.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.res4.4.conv3.weight\u001b[0m\n",
      "  \u001b[35mbackbone.res4.4.conv3.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.res4.5.conv1.weight\u001b[0m\n",
      "  \u001b[35mbackbone.res4.5.conv1.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.res4.5.conv2.weight\u001b[0m\n",
      "  \u001b[35mbackbone.res4.5.conv2.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.res4.5.conv3.weight\u001b[0m\n",
      "  \u001b[35mbackbone.res4.5.conv3.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.res5.0.shortcut.weight\u001b[0m\n",
      "  \u001b[35mbackbone.res5.0.shortcut.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.res5.0.conv1.weight\u001b[0m\n",
      "  \u001b[35mbackbone.res5.0.conv1.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.res5.0.conv2.weight\u001b[0m\n",
      "  \u001b[35mbackbone.res5.0.conv2.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.res5.0.conv3.weight\u001b[0m\n",
      "  \u001b[35mbackbone.res5.0.conv3.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.res5.1.conv1.weight\u001b[0m\n",
      "  \u001b[35mbackbone.res5.1.conv1.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.res5.1.conv2.weight\u001b[0m\n",
      "  \u001b[35mbackbone.res5.1.conv2.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.res5.1.conv3.weight\u001b[0m\n",
      "  \u001b[35mbackbone.res5.1.conv3.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.res5.2.conv1.weight\u001b[0m\n",
      "  \u001b[35mbackbone.res5.2.conv1.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.res5.2.conv2.weight\u001b[0m\n",
      "  \u001b[35mbackbone.res5.2.conv2.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.res5.2.conv3.weight\u001b[0m\n",
      "  \u001b[35mbackbone.res5.2.conv3.norm.{bias, running_mean, running_var, weight}\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "model = instantiate(cfg.model)\n",
    "DetectionCheckpointer(model).load(\"/home/mrajaraman/master-thesis-dragonfly/external/mask2former-dragonfly/output_lifeplan_b_512_sahi_tiled_v9_R50_1024_one_cycle_lr_5e-5_colour_augs_15k_iters/model_final.pth\")\n",
    "category_mapping={\"0\": \"head\", \"1\": \"torso\", \"2\": \"tail\", \"3\": \"wings\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3805c89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "be6ea1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction(img_path, threshold):\n",
    "    # Load image\n",
    "    pil_img = Image.open(img_path).convert(\"RGB\")\n",
    "    image_np = np.array(pil_img)  # (H, W, C), RGB\n",
    "\n",
    "    # Apply test-time augmentations\n",
    "    mapper = instantiate(cfg.dataloader.test.mapper)\n",
    "    aug = mapper.augmentations\n",
    "    aug_input = T.AugInput(image_np)\n",
    "    _ = aug(aug_input)\n",
    "    image_np = aug_input.image\n",
    "\n",
    "    # Convert to tensor\n",
    "    image = torch.as_tensor(image_np.astype(\"float32\").transpose(2, 0, 1))\n",
    "    inputs = {\"image\": image.to(device)}\n",
    "\n",
    "    # Forward pass\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        pred = model([inputs])\n",
    "\n",
    "    # Get instances\n",
    "    instances = pred[0][\"pred_instances\"].to(\"cpu\")\n",
    "\n",
    "    # Filter by threshold\n",
    "    keep = instances.scores > threshold\n",
    "    instances = instances[keep]\n",
    "\n",
    "    # Extract outputs\n",
    "    scores = instances.scores.numpy()\n",
    "    boxes = instances.pred_boxes.tensor.numpy()\n",
    "    masks = instances.pred_masks.numpy()\n",
    "    labels = instances.pred_classes.numpy()\n",
    "\n",
    "    # Map labels to class names\n",
    "    pred_class = [category_mapping[str(i)] for i in labels]\n",
    "    pred_boxes = [[(x1, y1), (x2, y2)] for (x1, y1, x2, y2) in boxes]\n",
    "\n",
    "    return masks, pred_boxes, pred_class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6a37bf37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09/19/2025 20:56:14 - INFO - detectron2.data.dataset_mapper -   [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(1024, 1024), max_size=1024)]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'pred_instances'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[48], line 33\u001b[0m\n\u001b[1;32m     30\u001b[0m       plt\u001b[38;5;241m.\u001b[39myticks([])\n\u001b[1;32m     31\u001b[0m       plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[0;32m---> 33\u001b[0m \u001b[43minstance_segmentation_api\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/home/mrajaraman/dataset/originals/img_1458477504.jpg\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;66;03m# Replace with your image path\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[48], line 16\u001b[0m, in \u001b[0;36minstance_segmentation_api\u001b[0;34m(img_path, threshold, text_th)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minstance_segmentation_api\u001b[39m(img_path, threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m, text_th\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m---> 16\u001b[0m       masks, boxes, pred_cls \u001b[38;5;241m=\u001b[39m \u001b[43mget_prediction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m       img \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mimread(img_path)\n\u001b[1;32m     18\u001b[0m       img \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(img, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2RGB)\n",
      "Cell \u001b[0;32mIn[47], line 23\u001b[0m, in \u001b[0;36mget_prediction\u001b[0;34m(img_path, threshold)\u001b[0m\n\u001b[1;32m     20\u001b[0m     pred \u001b[38;5;241m=\u001b[39m model([inputs])\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Get instances\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m instances \u001b[38;5;241m=\u001b[39m \u001b[43mpred\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpred_instances\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Filter by threshold\u001b[39;00m\n\u001b[1;32m     26\u001b[0m keep \u001b[38;5;241m=\u001b[39m instances\u001b[38;5;241m.\u001b[39mscores \u001b[38;5;241m>\u001b[39m threshold\n",
      "\u001b[0;31mKeyError\u001b[0m: 'pred_instances'"
     ]
    }
   ],
   "source": [
    "# def random_colour_masks(image): # Takes a binary mask as input\n",
    "#       colours = [[0, 255, 0],[0, 0, 255],[255, 0, 0],[0, 255, 255],[255, 255, 0],[255, 0, 255],[80, 70, 180],[250, 80,\n",
    "#                      190],[245, 145, 50],[70, 150, 250],[50, 190, 190]]\n",
    "#       # Extracting all the channels\n",
    "#       r = np.zeros_like(image).astype(np.uint8)\n",
    "#       g = np.zeros_like(image).astype(np.uint8)\n",
    "#       b = np.zeros_like(image).astype(np.uint8)\n",
    "#       # Assigning random colors to each instances of segmentation defferenciated by white  \n",
    "#       r[image == 1], g[image == 1], b[image == 1] = colours[random.randrange(0,10)]\n",
    "#       coloured_mask = np.stack([r, g, b], axis=2)\n",
    "\n",
    "#       return coloured_mask\n",
    "\n",
    "# Visualization function\n",
    "def instance_segmentation_api(img_path, threshold=0.5, text_th=1):\n",
    "      masks, boxes, pred_cls = get_prediction(img_path, threshold)\n",
    "      img = cv2.imread(img_path)\n",
    "      img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "      for i in range(len(masks)):\n",
    "           #rgb_mask = random_colour_masks(masks[i])\n",
    "      #      img = cv2.addWeighted(img, 1, rgb_mask, 0.5, 0)\n",
    "           xmin,ymin,xmax,ymax=int(boxes[i][0][0]), int(boxes[i][0][1]),int(boxes[i][1][0]), int(boxes[i][1][1])\n",
    "           cv2.rectangle(img, (xmin,ymin), (xmax+5,ymax-2), (0,255,0), 1)\n",
    "           cv2.putText(img, pred_cls[i], (xmin, ymin), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,255,0), text_th)\n",
    "\n",
    "      plt.figure(figsize=(20,30))\n",
    "      plt.imshow(img)\n",
    "      plt.xticks([])\n",
    "      plt.yticks([])\n",
    "      plt.show()\n",
    "\n",
    "instance_segmentation_api(\"/home/mrajaraman/dataset/originals/img_1458477504.jpg\")# Replace with your image path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "586b3641",
   "metadata": {},
   "outputs": [],
   "source": [
    "# category_names = list(category_mapping.values())\n",
    "image = np.array(\n",
    "    cv2.imread(\n",
    "        '/home/mrajaraman/dataset/originals/img_1458477504.jpg', \n",
    "        cv2.IMREAD_COLOR\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f562f22e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09/19/2025 19:07:26 - INFO - detectron2.data.dataset_mapper -   [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(1024, 1024), max_size=1024)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GeneralizedRCNN(\n",
       "  (backbone): FPN(\n",
       "    (fpn_lateral2): Conv2d(\n",
       "      256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "      (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (fpn_output2): Conv2d(\n",
       "      256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "      (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (fpn_lateral3): Conv2d(\n",
       "      512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "      (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (fpn_output3): Conv2d(\n",
       "      256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "      (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (fpn_lateral4): Conv2d(\n",
       "      1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "      (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (fpn_output4): Conv2d(\n",
       "      256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "      (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (fpn_lateral5): Conv2d(\n",
       "      2048, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "      (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (fpn_output5): Conv2d(\n",
       "      256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "      (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (top_block): LastLevelMaxPool()\n",
       "    (bottom_up): ResNet(\n",
       "      (stem): BasicStem(\n",
       "        (conv1): Conv2d(\n",
       "          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
       "          (norm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (res2): Sequential(\n",
       "        (0): BottleneckBlock(\n",
       "          (shortcut): Conv2d(\n",
       "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (conv1): Conv2d(\n",
       "            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (conv2): Conv2d(\n",
       "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (norm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (conv3): Conv2d(\n",
       "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): BottleneckBlock(\n",
       "          (conv1): Conv2d(\n",
       "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (conv2): Conv2d(\n",
       "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (norm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (conv3): Conv2d(\n",
       "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (2): BottleneckBlock(\n",
       "          (conv1): Conv2d(\n",
       "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (conv2): Conv2d(\n",
       "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (norm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (conv3): Conv2d(\n",
       "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (res3): Sequential(\n",
       "        (0): BottleneckBlock(\n",
       "          (shortcut): Conv2d(\n",
       "            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
       "            (norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (conv1): Conv2d(\n",
       "            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
       "            (norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (conv2): Conv2d(\n",
       "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (conv3): Conv2d(\n",
       "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): BottleneckBlock(\n",
       "          (conv1): Conv2d(\n",
       "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (conv2): Conv2d(\n",
       "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (conv3): Conv2d(\n",
       "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (2): BottleneckBlock(\n",
       "          (conv1): Conv2d(\n",
       "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (conv2): Conv2d(\n",
       "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (conv3): Conv2d(\n",
       "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (3): BottleneckBlock(\n",
       "          (conv1): Conv2d(\n",
       "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (conv2): Conv2d(\n",
       "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (conv3): Conv2d(\n",
       "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (res4): Sequential(\n",
       "        (0): BottleneckBlock(\n",
       "          (shortcut): Conv2d(\n",
       "            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
       "            (norm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (conv1): Conv2d(\n",
       "            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
       "            (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (conv2): Conv2d(\n",
       "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (conv3): Conv2d(\n",
       "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): BottleneckBlock(\n",
       "          (conv1): Conv2d(\n",
       "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (conv2): Conv2d(\n",
       "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (conv3): Conv2d(\n",
       "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (2): BottleneckBlock(\n",
       "          (conv1): Conv2d(\n",
       "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (conv2): Conv2d(\n",
       "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (conv3): Conv2d(\n",
       "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (3): BottleneckBlock(\n",
       "          (conv1): Conv2d(\n",
       "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (conv2): Conv2d(\n",
       "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (conv3): Conv2d(\n",
       "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (4): BottleneckBlock(\n",
       "          (conv1): Conv2d(\n",
       "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (conv2): Conv2d(\n",
       "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (conv3): Conv2d(\n",
       "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (5): BottleneckBlock(\n",
       "          (conv1): Conv2d(\n",
       "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (conv2): Conv2d(\n",
       "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (conv3): Conv2d(\n",
       "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (res5): Sequential(\n",
       "        (0): BottleneckBlock(\n",
       "          (shortcut): Conv2d(\n",
       "            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
       "            (norm): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (conv1): Conv2d(\n",
       "            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
       "            (norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (conv2): Conv2d(\n",
       "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (conv3): Conv2d(\n",
       "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): BottleneckBlock(\n",
       "          (conv1): Conv2d(\n",
       "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (conv2): Conv2d(\n",
       "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (conv3): Conv2d(\n",
       "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (2): BottleneckBlock(\n",
       "          (conv1): Conv2d(\n",
       "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (conv2): Conv2d(\n",
       "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (conv3): Conv2d(\n",
       "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (proposal_generator): RPN(\n",
       "    (rpn_head): StandardRPNHead(\n",
       "      (conv): Sequential(\n",
       "        (conv0): Conv2d(\n",
       "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "          (activation): ReLU()\n",
       "        )\n",
       "        (conv1): Conv2d(\n",
       "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "          (activation): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (anchor_generator): DefaultAnchorGenerator(\n",
       "      (cell_anchors): BufferList()\n",
       "    )\n",
       "  )\n",
       "  (roi_heads): StandardROIHeads(\n",
       "    (box_pooler): ROIPooler(\n",
       "      (level_poolers): ModuleList(\n",
       "        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n",
       "        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n",
       "        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n",
       "        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n",
       "      )\n",
       "    )\n",
       "    (box_head): FastRCNNConvFCHead(\n",
       "      (conv1): Conv2d(\n",
       "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (activation): ReLU()\n",
       "      )\n",
       "      (conv2): Conv2d(\n",
       "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (activation): ReLU()\n",
       "      )\n",
       "      (conv3): Conv2d(\n",
       "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (activation): ReLU()\n",
       "      )\n",
       "      (conv4): Conv2d(\n",
       "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (activation): ReLU()\n",
       "      )\n",
       "      (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "      (fc1): Linear(in_features=12544, out_features=1024, bias=True)\n",
       "      (fc_relu1): ReLU()\n",
       "    )\n",
       "    (box_predictor): FastRCNNOutputLayers(\n",
       "      (cls_score): Linear(in_features=1024, out_features=5, bias=True)\n",
       "      (bbox_pred): Linear(in_features=1024, out_features=16, bias=True)\n",
       "    )\n",
       "    (mask_pooler): ROIPooler(\n",
       "      (level_poolers): ModuleList(\n",
       "        (0): ROIAlign(output_size=(14, 14), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n",
       "        (1): ROIAlign(output_size=(14, 14), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n",
       "        (2): ROIAlign(output_size=(14, 14), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n",
       "        (3): ROIAlign(output_size=(14, 14), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n",
       "      )\n",
       "    )\n",
       "    (mask_head): MaskRCNNConvUpsampleHead(\n",
       "      (mask_fcn1): Conv2d(\n",
       "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (activation): ReLU()\n",
       "      )\n",
       "      (mask_fcn2): Conv2d(\n",
       "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (activation): ReLU()\n",
       "      )\n",
       "      (mask_fcn3): Conv2d(\n",
       "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (activation): ReLU()\n",
       "      )\n",
       "      (mask_fcn4): Conv2d(\n",
       "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (activation): ReLU()\n",
       "      )\n",
       "      (deconv): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (deconv_relu): ReLU()\n",
       "      (predictor): Conv2d(256, 4, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if isinstance(image, np.ndarray) and cfg.dataloader.train.mapper.image_format == \"BGR\":\n",
    "        # convert RGB image to BGR format\n",
    "        image = image[:, :, ::-1]\n",
    "height, width = image.shape[:2]\n",
    "mapper = instantiate(cfg.dataloader.test.mapper)\n",
    "aug = mapper.augmentations\n",
    "image = aug(T.AugInput(image)).apply_image(image)\n",
    "image = torch.as_tensor(image.astype(\"float32\").transpose(2, 0, 1))\n",
    "image = image.to(device)\n",
    "\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "808afc4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "        inputs = {\"image\": image, \"height\": height, \"width\": width}\n",
    "prediction_result = model([inputs])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ac4621d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'instances': Instances(num_instances=0, image_height=353, image_width=500, fields=[pred_boxes: Boxes(tensor([], device='cuda:0', size=(0, 4), grad_fn=<ReshapeAliasBackward0>)), scores: tensor([], device='cuda:0', grad_fn=<IndexBackward0>), pred_classes: tensor([], device='cuda:0', dtype=torch.int64), pred_masks: tensor([], device='cuda:0', size=(0, 353, 500), dtype=torch.bool)])}\n"
     ]
    }
   ],
   "source": [
    "print(prediction_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "abafecd0",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "Instances index out of range!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m img \u001b[38;5;241m=\u001b[39m \u001b[43mprediction_result\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minstances\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39m_fields[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpred_masks\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m      2\u001b[0m bbox \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of detected instances is \u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(prediction_result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minstances\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39m_fields[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpred_boxes\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtensor))\n",
      "File \u001b[0;32m~/conda/mask_rcnn/lib/python3.8/site-packages/detectron2/structures/instances.py:135\u001b[0m, in \u001b[0;36mInstances.__getitem__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(item) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mint\u001b[39m:\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m item \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m item \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 135\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInstances index out of range!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    137\u001b[0m         item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mslice\u001b[39m(item, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m))\n",
      "\u001b[0;31mIndexError\u001b[0m: Instances index out of range!"
     ]
    }
   ],
   "source": [
    "img = prediction_result['instances'][0]._fields['pred_masks'].cpu().detach().numpy()\n",
    "bbox = []\n",
    "print(\"Number of detected instances is \", len(prediction_result['instances']._fields['pred_boxes'].tensor))\n",
    "for i in prediction_result['instances']._fields['pred_boxes'].tensor:\n",
    "        print(\"i is \", i)\n",
    "        bbox.append(i.cpu().detach().numpy())\n",
    "\n",
    "# Display the image\n",
    "plt.imshow(Image.open('/home/mrajaraman/dataset/originals/img_1458477504.jpg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ded9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(bbox)):\n",
    "        # print(\"length is \", len(bbox))\n",
    "        plt.gca().add_patch(Rectangle((bbox[i][0], bbox[i][1]), bbox[i][2]-bbox[i][0], bbox[i][3]-bbox[i][1], linewidth=1, edgecolor='r', facecolor='none'))\n",
    "        print(f\"Instance {i+1} added to image\")\n",
    "\n",
    "plt.title(\"Inference Result of MaskRCNN on image\")\n",
    "plt.savefig(\"trained_inference_maskrcnn.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56e86d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(aug)\n",
    "print()\n",
    "\n",
    "# Output sample mask predictions:\n",
    "sample_preds = prediction_result['instances'][0]._fields['pred_masks'].cpu().detach().numpy()\n",
    "print(sample_preds)\n",
    "print(sample_preds.shape)\n",
    "print(sample_preds.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "16988752",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09/19/2025 22:43:22 - INFO - detectron2.checkpoint.detection_checkpoint -   [DetectionCheckpointer] Loading from /home/mrajaraman/master-thesis-dragonfly/external/mask2former-dragonfly/output_lifeplan_b_512_sahi_tiled_v9_R50_1024_one_cycle_lr_5e-5_colour_augs_15k_iters/model_final.pth ...\n",
      "09/19/2025 22:43:22 - INFO - fvcore.common.checkpoint -   [Checkpointer] Loading from /home/mrajaraman/master-thesis-dragonfly/external/mask2former-dragonfly/output_lifeplan_b_512_sahi_tiled_v9_R50_1024_one_cycle_lr_5e-5_colour_augs_15k_iters/model_final.pth ...\n",
      "09/19/2025 22:43:22 - WARNING - fvcore.common.checkpoint -   Some model parameters or buffers are not found in the checkpoint:\n",
      "\u001b[34mbackbone.bottom_up.res2.0.conv1.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res2.0.conv1.weight\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res2.0.conv2.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res2.0.conv2.weight\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res2.0.conv3.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res2.0.conv3.weight\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res2.0.shortcut.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res2.0.shortcut.weight\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res2.1.conv1.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res2.1.conv1.weight\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res2.1.conv2.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res2.1.conv2.weight\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res2.1.conv3.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res2.1.conv3.weight\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res2.2.conv1.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res2.2.conv1.weight\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res2.2.conv2.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res2.2.conv2.weight\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res2.2.conv3.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res2.2.conv3.weight\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res3.0.conv1.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res3.0.conv1.weight\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res3.0.conv2.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res3.0.conv2.weight\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res3.0.conv3.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res3.0.conv3.weight\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res3.0.shortcut.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res3.0.shortcut.weight\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res3.1.conv1.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res3.1.conv1.weight\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res3.1.conv2.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res3.1.conv2.weight\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res3.1.conv3.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res3.1.conv3.weight\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res3.2.conv1.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res3.2.conv1.weight\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res3.2.conv2.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res3.2.conv2.weight\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res3.2.conv3.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res3.2.conv3.weight\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res3.3.conv1.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res3.3.conv1.weight\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res3.3.conv2.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res3.3.conv2.weight\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res3.3.conv3.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res3.3.conv3.weight\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res4.0.conv1.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res4.0.conv1.weight\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res4.0.conv2.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res4.0.conv2.weight\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res4.0.conv3.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res4.0.conv3.weight\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res4.0.shortcut.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res4.0.shortcut.weight\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res4.1.conv1.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res4.1.conv1.weight\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res4.1.conv2.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res4.1.conv2.weight\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res4.1.conv3.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res4.1.conv3.weight\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res4.2.conv1.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res4.2.conv1.weight\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res4.2.conv2.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res4.2.conv2.weight\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res4.2.conv3.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res4.2.conv3.weight\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res4.3.conv1.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res4.3.conv1.weight\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res4.3.conv2.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res4.3.conv2.weight\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res4.3.conv3.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res4.3.conv3.weight\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res4.4.conv1.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res4.4.conv1.weight\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res4.4.conv2.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res4.4.conv2.weight\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res4.4.conv3.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res4.4.conv3.weight\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res4.5.conv1.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res4.5.conv1.weight\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res4.5.conv2.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res4.5.conv2.weight\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res4.5.conv3.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res4.5.conv3.weight\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res5.0.conv1.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res5.0.conv1.weight\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res5.0.conv2.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res5.0.conv2.weight\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res5.0.conv3.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res5.0.conv3.weight\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res5.0.shortcut.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res5.0.shortcut.weight\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res5.1.conv1.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res5.1.conv1.weight\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res5.1.conv2.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res5.1.conv2.weight\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res5.1.conv3.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res5.1.conv3.weight\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res5.2.conv1.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res5.2.conv1.weight\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res5.2.conv2.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res5.2.conv2.weight\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res5.2.conv3.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.res5.2.conv3.weight\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.stem.conv1.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.stem.conv1.weight\u001b[0m\n",
      "\u001b[34mbackbone.fpn_lateral2.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "\u001b[34mbackbone.fpn_lateral2.weight\u001b[0m\n",
      "\u001b[34mbackbone.fpn_lateral3.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "\u001b[34mbackbone.fpn_lateral3.weight\u001b[0m\n",
      "\u001b[34mbackbone.fpn_lateral4.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "\u001b[34mbackbone.fpn_lateral4.weight\u001b[0m\n",
      "\u001b[34mbackbone.fpn_lateral5.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "\u001b[34mbackbone.fpn_lateral5.weight\u001b[0m\n",
      "\u001b[34mbackbone.fpn_output2.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "\u001b[34mbackbone.fpn_output2.weight\u001b[0m\n",
      "\u001b[34mbackbone.fpn_output3.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "\u001b[34mbackbone.fpn_output3.weight\u001b[0m\n",
      "\u001b[34mbackbone.fpn_output4.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "\u001b[34mbackbone.fpn_output4.weight\u001b[0m\n",
      "\u001b[34mbackbone.fpn_output5.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "\u001b[34mbackbone.fpn_output5.weight\u001b[0m\n",
      "\u001b[34mproposal_generator.rpn_head.anchor_deltas.{bias, weight}\u001b[0m\n",
      "\u001b[34mproposal_generator.rpn_head.conv.conv0.{bias, weight}\u001b[0m\n",
      "\u001b[34mproposal_generator.rpn_head.conv.conv1.{bias, weight}\u001b[0m\n",
      "\u001b[34mproposal_generator.rpn_head.objectness_logits.{bias, weight}\u001b[0m\n",
      "\u001b[34mroi_heads.box_head.conv1.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "\u001b[34mroi_heads.box_head.conv1.weight\u001b[0m\n",
      "\u001b[34mroi_heads.box_head.conv2.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "\u001b[34mroi_heads.box_head.conv2.weight\u001b[0m\n",
      "\u001b[34mroi_heads.box_head.conv3.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "\u001b[34mroi_heads.box_head.conv3.weight\u001b[0m\n",
      "\u001b[34mroi_heads.box_head.conv4.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "\u001b[34mroi_heads.box_head.conv4.weight\u001b[0m\n",
      "\u001b[34mroi_heads.box_head.fc1.{bias, weight}\u001b[0m\n",
      "\u001b[34mroi_heads.box_predictor.bbox_pred.{bias, weight}\u001b[0m\n",
      "\u001b[34mroi_heads.box_predictor.cls_score.{bias, weight}\u001b[0m\n",
      "\u001b[34mroi_heads.mask_head.deconv.{bias, weight}\u001b[0m\n",
      "\u001b[34mroi_heads.mask_head.mask_fcn1.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "\u001b[34mroi_heads.mask_head.mask_fcn1.weight\u001b[0m\n",
      "\u001b[34mroi_heads.mask_head.mask_fcn2.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "\u001b[34mroi_heads.mask_head.mask_fcn2.weight\u001b[0m\n",
      "\u001b[34mroi_heads.mask_head.mask_fcn3.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "\u001b[34mroi_heads.mask_head.mask_fcn3.weight\u001b[0m\n",
      "\u001b[34mroi_heads.mask_head.mask_fcn4.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "\u001b[34mroi_heads.mask_head.mask_fcn4.weight\u001b[0m\n",
      "\u001b[34mroi_heads.mask_head.predictor.{bias, weight}\u001b[0m\n",
      "09/19/2025 22:43:22 - WARNING - fvcore.common.checkpoint -   The checkpoint state_dict contains keys that are not used by the model:\n",
      "  \u001b[35msem_seg_head.pixel_decoder.input_proj.0.0.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.pixel_decoder.input_proj.0.1.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.pixel_decoder.input_proj.1.0.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.pixel_decoder.input_proj.1.1.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.pixel_decoder.input_proj.2.0.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.pixel_decoder.input_proj.2.1.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.pixel_decoder.transformer.level_embed\u001b[0m\n",
      "  \u001b[35msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.sampling_offsets.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.attention_weights.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.value_proj.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.output_proj.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.pixel_decoder.transformer.encoder.layers.0.norm1.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.pixel_decoder.transformer.encoder.layers.0.linear1.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.pixel_decoder.transformer.encoder.layers.0.linear2.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.pixel_decoder.transformer.encoder.layers.0.norm2.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.sampling_offsets.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.attention_weights.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.value_proj.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.output_proj.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.pixel_decoder.transformer.encoder.layers.1.norm1.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.pixel_decoder.transformer.encoder.layers.1.linear1.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.pixel_decoder.transformer.encoder.layers.1.linear2.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.pixel_decoder.transformer.encoder.layers.1.norm2.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.sampling_offsets.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.attention_weights.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.value_proj.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.output_proj.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.pixel_decoder.transformer.encoder.layers.2.norm1.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.pixel_decoder.transformer.encoder.layers.2.linear1.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.pixel_decoder.transformer.encoder.layers.2.linear2.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.pixel_decoder.transformer.encoder.layers.2.norm2.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.sampling_offsets.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.attention_weights.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.value_proj.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.output_proj.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.pixel_decoder.transformer.encoder.layers.3.norm1.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.pixel_decoder.transformer.encoder.layers.3.linear1.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.pixel_decoder.transformer.encoder.layers.3.linear2.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.pixel_decoder.transformer.encoder.layers.3.norm2.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.sampling_offsets.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.attention_weights.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.value_proj.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.output_proj.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.pixel_decoder.transformer.encoder.layers.4.norm1.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.pixel_decoder.transformer.encoder.layers.4.linear1.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.pixel_decoder.transformer.encoder.layers.4.linear2.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.pixel_decoder.transformer.encoder.layers.4.norm2.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.sampling_offsets.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.attention_weights.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.value_proj.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.output_proj.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.pixel_decoder.transformer.encoder.layers.5.norm1.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.pixel_decoder.transformer.encoder.layers.5.linear1.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.pixel_decoder.transformer.encoder.layers.5.linear2.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.pixel_decoder.transformer.encoder.layers.5.norm2.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.pixel_decoder.mask_features.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.pixel_decoder.adapter_1.weight\u001b[0m\n",
      "  \u001b[35msem_seg_head.pixel_decoder.adapter_1.norm.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.pixel_decoder.layer_1.weight\u001b[0m\n",
      "  \u001b[35msem_seg_head.pixel_decoder.layer_1.norm.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.predictor.transformer_self_attention_layers.0.self_attn.{in_proj_bias, in_proj_weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.predictor.transformer_self_attention_layers.0.self_attn.out_proj.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.predictor.transformer_self_attention_layers.0.norm.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.predictor.transformer_self_attention_layers.1.self_attn.{in_proj_bias, in_proj_weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.predictor.transformer_self_attention_layers.1.self_attn.out_proj.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.predictor.transformer_self_attention_layers.1.norm.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.predictor.transformer_self_attention_layers.2.self_attn.{in_proj_bias, in_proj_weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.predictor.transformer_self_attention_layers.2.self_attn.out_proj.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.predictor.transformer_self_attention_layers.2.norm.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.predictor.transformer_self_attention_layers.3.self_attn.{in_proj_bias, in_proj_weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.predictor.transformer_self_attention_layers.3.self_attn.out_proj.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.predictor.transformer_self_attention_layers.3.norm.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.predictor.transformer_self_attention_layers.4.self_attn.{in_proj_bias, in_proj_weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.predictor.transformer_self_attention_layers.4.self_attn.out_proj.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.predictor.transformer_self_attention_layers.4.norm.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.predictor.transformer_self_attention_layers.5.self_attn.{in_proj_bias, in_proj_weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.predictor.transformer_self_attention_layers.5.self_attn.out_proj.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.predictor.transformer_self_attention_layers.5.norm.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.predictor.transformer_self_attention_layers.6.self_attn.{in_proj_bias, in_proj_weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.predictor.transformer_self_attention_layers.6.self_attn.out_proj.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.predictor.transformer_self_attention_layers.6.norm.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.predictor.transformer_self_attention_layers.7.self_attn.{in_proj_bias, in_proj_weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.predictor.transformer_self_attention_layers.7.self_attn.out_proj.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.predictor.transformer_self_attention_layers.7.norm.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.predictor.transformer_self_attention_layers.8.self_attn.{in_proj_bias, in_proj_weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.predictor.transformer_self_attention_layers.8.self_attn.out_proj.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.predictor.transformer_self_attention_layers.8.norm.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.predictor.transformer_cross_attention_layers.0.multihead_attn.{in_proj_bias, in_proj_weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.predictor.transformer_cross_attention_layers.0.multihead_attn.out_proj.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.predictor.transformer_cross_attention_layers.0.norm.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.predictor.transformer_cross_attention_layers.1.multihead_attn.{in_proj_bias, in_proj_weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.predictor.transformer_cross_attention_layers.1.multihead_attn.out_proj.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.predictor.transformer_cross_attention_layers.1.norm.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.predictor.transformer_cross_attention_layers.2.multihead_attn.{in_proj_bias, in_proj_weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.predictor.transformer_cross_attention_layers.2.multihead_attn.out_proj.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.predictor.transformer_cross_attention_layers.2.norm.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.predictor.transformer_cross_attention_layers.3.multihead_attn.{in_proj_bias, in_proj_weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.predictor.transformer_cross_attention_layers.3.multihead_attn.out_proj.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.predictor.transformer_cross_attention_layers.3.norm.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.predictor.transformer_cross_attention_layers.4.multihead_attn.{in_proj_bias, in_proj_weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.predictor.transformer_cross_attention_layers.4.multihead_attn.out_proj.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.predictor.transformer_cross_attention_layers.4.norm.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.predictor.transformer_cross_attention_layers.5.multihead_attn.{in_proj_bias, in_proj_weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.predictor.transformer_cross_attention_layers.5.multihead_attn.out_proj.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.predictor.transformer_cross_attention_layers.5.norm.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.predictor.transformer_cross_attention_layers.6.multihead_attn.{in_proj_bias, in_proj_weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.predictor.transformer_cross_attention_layers.6.multihead_attn.out_proj.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.predictor.transformer_cross_attention_layers.6.norm.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.predictor.transformer_cross_attention_layers.7.multihead_attn.{in_proj_bias, in_proj_weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.predictor.transformer_cross_attention_layers.7.multihead_attn.out_proj.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.predictor.transformer_cross_attention_layers.7.norm.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.predictor.transformer_cross_attention_layers.8.multihead_attn.{in_proj_bias, in_proj_weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.predictor.transformer_cross_attention_layers.8.multihead_attn.out_proj.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.predictor.transformer_cross_attention_layers.8.norm.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.predictor.transformer_ffn_layers.0.linear1.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.predictor.transformer_ffn_layers.0.linear2.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.predictor.transformer_ffn_layers.0.norm.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.predictor.transformer_ffn_layers.1.linear1.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.predictor.transformer_ffn_layers.1.linear2.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.predictor.transformer_ffn_layers.1.norm.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.predictor.transformer_ffn_layers.2.linear1.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.predictor.transformer_ffn_layers.2.linear2.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.predictor.transformer_ffn_layers.2.norm.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.predictor.transformer_ffn_layers.3.linear1.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.predictor.transformer_ffn_layers.3.linear2.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.predictor.transformer_ffn_layers.3.norm.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.predictor.transformer_ffn_layers.4.linear1.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.predictor.transformer_ffn_layers.4.linear2.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.predictor.transformer_ffn_layers.4.norm.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.predictor.transformer_ffn_layers.5.linear1.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.predictor.transformer_ffn_layers.5.linear2.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.predictor.transformer_ffn_layers.5.norm.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.predictor.transformer_ffn_layers.6.linear1.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.predictor.transformer_ffn_layers.6.linear2.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.predictor.transformer_ffn_layers.6.norm.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.predictor.transformer_ffn_layers.7.linear1.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.predictor.transformer_ffn_layers.7.linear2.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.predictor.transformer_ffn_layers.7.norm.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.predictor.transformer_ffn_layers.8.linear1.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.predictor.transformer_ffn_layers.8.linear2.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.predictor.transformer_ffn_layers.8.norm.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.predictor.decoder_norm.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.predictor.query_feat.weight\u001b[0m\n",
      "  \u001b[35msem_seg_head.predictor.query_embed.weight\u001b[0m\n",
      "  \u001b[35msem_seg_head.predictor.level_embed.weight\u001b[0m\n",
      "  \u001b[35msem_seg_head.predictor.class_embed.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.predictor.mask_embed.layers.0.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.predictor.mask_embed.layers.1.{bias, weight}\u001b[0m\n",
      "  \u001b[35msem_seg_head.predictor.mask_embed.layers.2.{bias, weight}\u001b[0m\n",
      "  \u001b[35mcriterion.empty_weight\u001b[0m\n",
      "  \u001b[35mbackbone.stem.conv1.weight\u001b[0m\n",
      "  \u001b[35mbackbone.stem.conv1.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.res2.0.shortcut.weight\u001b[0m\n",
      "  \u001b[35mbackbone.res2.0.shortcut.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.res2.0.conv1.weight\u001b[0m\n",
      "  \u001b[35mbackbone.res2.0.conv1.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.res2.0.conv2.weight\u001b[0m\n",
      "  \u001b[35mbackbone.res2.0.conv2.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.res2.0.conv3.weight\u001b[0m\n",
      "  \u001b[35mbackbone.res2.0.conv3.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.res2.1.conv1.weight\u001b[0m\n",
      "  \u001b[35mbackbone.res2.1.conv1.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.res2.1.conv2.weight\u001b[0m\n",
      "  \u001b[35mbackbone.res2.1.conv2.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.res2.1.conv3.weight\u001b[0m\n",
      "  \u001b[35mbackbone.res2.1.conv3.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.res2.2.conv1.weight\u001b[0m\n",
      "  \u001b[35mbackbone.res2.2.conv1.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.res2.2.conv2.weight\u001b[0m\n",
      "  \u001b[35mbackbone.res2.2.conv2.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.res2.2.conv3.weight\u001b[0m\n",
      "  \u001b[35mbackbone.res2.2.conv3.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.res3.0.shortcut.weight\u001b[0m\n",
      "  \u001b[35mbackbone.res3.0.shortcut.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.res3.0.conv1.weight\u001b[0m\n",
      "  \u001b[35mbackbone.res3.0.conv1.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.res3.0.conv2.weight\u001b[0m\n",
      "  \u001b[35mbackbone.res3.0.conv2.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.res3.0.conv3.weight\u001b[0m\n",
      "  \u001b[35mbackbone.res3.0.conv3.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.res3.1.conv1.weight\u001b[0m\n",
      "  \u001b[35mbackbone.res3.1.conv1.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.res3.1.conv2.weight\u001b[0m\n",
      "  \u001b[35mbackbone.res3.1.conv2.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.res3.1.conv3.weight\u001b[0m\n",
      "  \u001b[35mbackbone.res3.1.conv3.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.res3.2.conv1.weight\u001b[0m\n",
      "  \u001b[35mbackbone.res3.2.conv1.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.res3.2.conv2.weight\u001b[0m\n",
      "  \u001b[35mbackbone.res3.2.conv2.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.res3.2.conv3.weight\u001b[0m\n",
      "  \u001b[35mbackbone.res3.2.conv3.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.res3.3.conv1.weight\u001b[0m\n",
      "  \u001b[35mbackbone.res3.3.conv1.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.res3.3.conv2.weight\u001b[0m\n",
      "  \u001b[35mbackbone.res3.3.conv2.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.res3.3.conv3.weight\u001b[0m\n",
      "  \u001b[35mbackbone.res3.3.conv3.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.res4.0.shortcut.weight\u001b[0m\n",
      "  \u001b[35mbackbone.res4.0.shortcut.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.res4.0.conv1.weight\u001b[0m\n",
      "  \u001b[35mbackbone.res4.0.conv1.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.res4.0.conv2.weight\u001b[0m\n",
      "  \u001b[35mbackbone.res4.0.conv2.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.res4.0.conv3.weight\u001b[0m\n",
      "  \u001b[35mbackbone.res4.0.conv3.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.res4.1.conv1.weight\u001b[0m\n",
      "  \u001b[35mbackbone.res4.1.conv1.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.res4.1.conv2.weight\u001b[0m\n",
      "  \u001b[35mbackbone.res4.1.conv2.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.res4.1.conv3.weight\u001b[0m\n",
      "  \u001b[35mbackbone.res4.1.conv3.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.res4.2.conv1.weight\u001b[0m\n",
      "  \u001b[35mbackbone.res4.2.conv1.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.res4.2.conv2.weight\u001b[0m\n",
      "  \u001b[35mbackbone.res4.2.conv2.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.res4.2.conv3.weight\u001b[0m\n",
      "  \u001b[35mbackbone.res4.2.conv3.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.res4.3.conv1.weight\u001b[0m\n",
      "  \u001b[35mbackbone.res4.3.conv1.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.res4.3.conv2.weight\u001b[0m\n",
      "  \u001b[35mbackbone.res4.3.conv2.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.res4.3.conv3.weight\u001b[0m\n",
      "  \u001b[35mbackbone.res4.3.conv3.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.res4.4.conv1.weight\u001b[0m\n",
      "  \u001b[35mbackbone.res4.4.conv1.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.res4.4.conv2.weight\u001b[0m\n",
      "  \u001b[35mbackbone.res4.4.conv2.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.res4.4.conv3.weight\u001b[0m\n",
      "  \u001b[35mbackbone.res4.4.conv3.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.res4.5.conv1.weight\u001b[0m\n",
      "  \u001b[35mbackbone.res4.5.conv1.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.res4.5.conv2.weight\u001b[0m\n",
      "  \u001b[35mbackbone.res4.5.conv2.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.res4.5.conv3.weight\u001b[0m\n",
      "  \u001b[35mbackbone.res4.5.conv3.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.res5.0.shortcut.weight\u001b[0m\n",
      "  \u001b[35mbackbone.res5.0.shortcut.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.res5.0.conv1.weight\u001b[0m\n",
      "  \u001b[35mbackbone.res5.0.conv1.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.res5.0.conv2.weight\u001b[0m\n",
      "  \u001b[35mbackbone.res5.0.conv2.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.res5.0.conv3.weight\u001b[0m\n",
      "  \u001b[35mbackbone.res5.0.conv3.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.res5.1.conv1.weight\u001b[0m\n",
      "  \u001b[35mbackbone.res5.1.conv1.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.res5.1.conv2.weight\u001b[0m\n",
      "  \u001b[35mbackbone.res5.1.conv2.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.res5.1.conv3.weight\u001b[0m\n",
      "  \u001b[35mbackbone.res5.1.conv3.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.res5.2.conv1.weight\u001b[0m\n",
      "  \u001b[35mbackbone.res5.2.conv1.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.res5.2.conv2.weight\u001b[0m\n",
      "  \u001b[35mbackbone.res5.2.conv2.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.res5.2.conv3.weight\u001b[0m\n",
      "  \u001b[35mbackbone.res5.2.conv3.norm.{bias, running_mean, running_var, weight}\u001b[0m\n",
      "09/19/2025 22:43:22 - INFO - detectron2.data.dataset_mapper -   [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(1024, 1024), max_size=1024)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'instances': Instances(num_instances=6, image_height=353, image_width=500, fields=[pred_boxes: Boxes(tensor([[  0.0000,   0.0000, 500.0000, 353.0000],\n",
      "        [177.4247,   0.0000, 177.5701, 353.0000],\n",
      "        [359.6880,   0.0000, 363.4079, 353.0000],\n",
      "        [369.2262,   0.0000, 500.0000, 353.0000],\n",
      "        [248.7891,   0.0000, 252.8632, 353.0000],\n",
      "        [424.3461, 294.9991, 424.3470, 353.0000]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)), scores: tensor([1., 1., 1., 1., 1., 1.], device='cuda:0', grad_fn=<IndexBackward0>), pred_classes: tensor([1, 1, 1, 1, 1, 3], device='cuda:0'), pred_masks: tensor([[[False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         ...,\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False]],\n",
      "\n",
      "        [[False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         ...,\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False]],\n",
      "\n",
      "        [[False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         ...,\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False]],\n",
      "\n",
      "        [[False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         ...,\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False]],\n",
      "\n",
      "        [[False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         ...,\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False]],\n",
      "\n",
      "        [[False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         ...,\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False]]], device='cuda:0')])}\n",
      "Number of detected instances is  6\n",
      "i is  tensor([  0.,   0., 500., 353.], device='cuda:0', grad_fn=<UnbindBackward0>)\n",
      "i is  tensor([177.4247,   0.0000, 177.5701, 353.0000], device='cuda:0',\n",
      "       grad_fn=<UnbindBackward0>)\n",
      "i is  tensor([359.6880,   0.0000, 363.4079, 353.0000], device='cuda:0',\n",
      "       grad_fn=<UnbindBackward0>)\n",
      "i is  tensor([369.2262,   0.0000, 500.0000, 353.0000], device='cuda:0',\n",
      "       grad_fn=<UnbindBackward0>)\n",
      "i is  tensor([248.7891,   0.0000, 252.8632, 353.0000], device='cuda:0',\n",
      "       grad_fn=<UnbindBackward0>)\n",
      "i is  tensor([424.3461, 294.9991, 424.3470, 353.0000], device='cuda:0',\n",
      "       grad_fn=<UnbindBackward0>)\n",
      "Instance 1 added to image\n",
      "Instance 2 added to image\n",
      "Instance 3 added to image\n",
      "Instance 4 added to image\n",
      "Instance 5 added to image\n",
      "Instance 6 added to image\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AugmentationList[ResizeShortestEdge(short_edge_length=(1024, 1024), max_size=1024)]\n",
      "\n",
      "[[[False False False ... False False False]\n",
      "  [False False False ... False False False]\n",
      "  [False False False ... False False False]\n",
      "  ...\n",
      "  [False False False ... False False False]\n",
      "  [False False False ... False False False]\n",
      "  [False False False ... False False False]]]\n",
      "(1, 353, 500)\n",
      "bool\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n>>> print(prediction_result['instances'][0]._fields['pred_masks'])\\ntensor([[[0., 0., 0.,  ..., 0., 0., 0.],\\n         [0., 0., 0.,  ..., 0., 0., 0.],\\n         [0., 0., 0.,  ..., 0., 0., 0.],\\n         ...,\\n         [0., 0., 0.,  ..., 0., 0., 0.],\\n         [0., 0., 0.,  ..., 0., 0., 0.],\\n         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')\\n>>> print(prediction_result['instances'][0]._fields['pred_masks'].size())\\ntorch.Size([1, 256, 256])\\n\\n- Note: we can see here that the output mask predictions are a float32 array that \\n        is the same size as the sample image that we are predicting on.\\n>>> np.unique(sample_preds)\\narray([0., 1.], dtype=float32)\\n- Appears to be in RLE format -> indicating we have an RLE mask that by definition is pixel-based,\\nso we can't have sub-pixel mask coordinates:\\n\\nThe process is:\\n\\nUpsample by 5x and round to nearest integer using +.5 trick\\nGet dense boundary points at this higher resolution\\nDownsample back by dividing by scale\\nApply floor/ceil and boundary checks\\nConvert to final integer coordinates\\nSo decimal coordinates are first scaled up for better precision during boundary calculation, but ultimately get converted to integers through this upscale-then-downscale process with rounding.\\n\\nThis explains why super-resolution could help - it effectively increases the resolution at which this rounding occurs, allowing for more precise boundary definitions.\\n\\n\\n\""
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import logging\n",
    "from typing import List, Optional\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from sahi.models.base import DetectionModel\n",
    "from sahi.prediction import ObjectPrediction\n",
    "from sahi.utils.cv import get_bbox_from_bool_mask, get_coco_segmentation_from_bool_mask\n",
    "from sahi.utils.import_utils import check_requirements\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "from detectron2.data import MetadataCatalog\n",
    "from detectron2.engine.defaults import DefaultPredictor\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.data import MetadataCatalog\n",
    "from detectron2.projects.deeplab import add_deeplab_config\n",
    "import cv2\n",
    "from detectron2.config import LazyConfig, instantiate\n",
    "from detectron2.checkpoint import DetectionCheckpointer\n",
    "import detectron2.data.transforms as T\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "cfg = get_cfg()\n",
    "cfg.set_new_allowed(True) \n",
    "cfg = LazyConfig.load(\"/home/mrajaraman/master-thesis-dragonfly/external/mask-rcnn-dragonfly/configs/new_baselines/mask_rcnn_R_50_FPN_100ep_LSJ.py\") \n",
    "# cfg.train.init_checkpoint = \"/h/jquinto/Mask-RCNN/model_final_14d201.pkl\"\n",
    "\n",
    "# set model device\n",
    "# cfg.MODEL.DEVICE = \"self.device.type\"\n",
    "cfg.train.device = \"cuda\"\n",
    "\n",
    "# # set input image size\n",
    "# # NEW TRAINING PIPELINE\n",
    "# cfg.INPUT.MIN_SIZE_TEST = 1024\n",
    "# cfg.INPUT.MAX_SIZE_TEST = 1024\n",
    "# cfg.freeze()\n",
    "\n",
    "# init predictor\n",
    "model = instantiate(cfg.model)\n",
    "DetectionCheckpointer(model).load(\"/home/mrajaraman/master-thesis-dragonfly/external/mask2former-dragonfly/output_lifeplan_b_512_sahi_tiled_v9_R50_1024_one_cycle_lr_5e-5_colour_augs_15k_iters/model_final.pth\")\n",
    "# category_mapping={\"1\": \"b\"}\n",
    "\n",
    "# detectron2 category mapping\n",
    "category_names = list(category_mapping.values())\n",
    "image = np.array(\n",
    "    cv2.imread(\n",
    "        \"/home/mrajaraman/dataset/originals/img_1458477504.jpg\", \n",
    "        cv2.IMREAD_COLOR\n",
    "))\n",
    "\n",
    "if isinstance(image, np.ndarray) and cfg.dataloader.train.mapper.image_format == \"BGR\":\n",
    "        # convert RGB image to BGR format\n",
    "        image = image[:, :, ::-1]\n",
    "height, width = image.shape[:2]\n",
    "mapper = instantiate(cfg.dataloader.test.mapper)\n",
    "aug = mapper.augmentations\n",
    "image = aug(T.AugInput(image)).apply_image(image)\n",
    "image = torch.as_tensor(image.astype(\"float32\").transpose(2, 0, 1))\n",
    "image = image.to(device)\n",
    "\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "        inputs = {\"image\": image, \"height\": height, \"width\": width}\n",
    "prediction_result = model([inputs])[0]\n",
    "print(prediction_result)\n",
    "\n",
    "img = prediction_result['instances'][0]._fields['pred_masks'].cpu().detach().numpy()\n",
    "bbox = []\n",
    "print(\"Number of detected instances is \", len(prediction_result['instances']._fields['pred_boxes'].tensor))\n",
    "for i in prediction_result['instances']._fields['pred_boxes'].tensor:\n",
    "        print(\"i is \", i)\n",
    "        bbox.append(i.cpu().detach().numpy())\n",
    "\n",
    "for i in range(len(bbox)):\n",
    "        # print(\"length is \", len(bbox))\n",
    "        plt.gca().add_patch(Rectangle((bbox[i][0], bbox[i][1]), bbox[i][2]-bbox[i][0], bbox[i][3]-bbox[i][1], linewidth=1, edgecolor='r', facecolor='none'))\n",
    "        print(f\"Instance {i+1} added to image\")\n",
    "\n",
    "plt.title(\"Inference Result of MaskRCNN on image\")\n",
    "plt.show()\n",
    "# plt.savefig(\"trained_inference_maskrcnn.png\")\n",
    "\n",
    "# image = np.ascontiguousarray(image).copy()\n",
    "# img = torch.from_numpy(image)\n",
    "# img = img.permute(2, 0, 1)  # HWC -> CHW\n",
    "\n",
    "# if torch.cuda.is_available():\n",
    "#         img = img.cuda()\n",
    "# inputs = [{\"image\": img}]\n",
    "\n",
    "# # run the model\n",
    "# model.to(device)\n",
    "# model.eval()\n",
    "# with torch.no_grad():\n",
    "#         predictions_ls = model(inputs)\n",
    "# prediction_result = predictions_ls[0]\n",
    "# print(prediction_result)\n",
    "# original_predictions = prediction_result\n",
    "\n",
    "# PROOF THAT RESIZING WORKS AS EXPECTED DURING INFERENCE\n",
    "print(aug)\n",
    "print()\n",
    "\n",
    "# Output sample mask predictions:\n",
    "sample_preds = prediction_result['instances'][0]._fields['pred_masks'].cpu().detach().numpy()\n",
    "print(sample_preds)\n",
    "print(sample_preds.shape)\n",
    "print(sample_preds.dtype)\n",
    "\n",
    "\"\"\"\n",
    ">>> print(prediction_result['instances'][0]._fields['pred_masks'])\n",
    "tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
    "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
    "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
    "         ...,\n",
    "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
    "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
    "         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')\n",
    ">>> print(prediction_result['instances'][0]._fields['pred_masks'].size())\n",
    "torch.Size([1, 256, 256])\n",
    "\n",
    "- Note: we can see here that the output mask predictions are a float32 array that \n",
    "        is the same size as the sample image that we are predicting on.\n",
    ">>> np.unique(sample_preds)\n",
    "array([0., 1.], dtype=float32)\n",
    "- Appears to be in RLE format -> indicating we have an RLE mask that by definition is pixel-based,\n",
    "so we can't have sub-pixel mask coordinates:\n",
    "\n",
    "The process is:\n",
    "\n",
    "Upsample by 5x and round to nearest integer using +.5 trick\n",
    "Get dense boundary points at this higher resolution\n",
    "Downsample back by dividing by scale\n",
    "Apply floor/ceil and boundary checks\n",
    "Convert to final integer coordinates\n",
    "So decimal coordinates are first scaled up for better precision during boundary calculation, but ultimately get converted to integers through this upscale-then-downscale process with rounding.\n",
    "\n",
    "This explains why super-resolution could help - it effectively increases the resolution at which this rounding occurs, allowing for more precise boundary definitions.\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05af27f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mask_rcnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
